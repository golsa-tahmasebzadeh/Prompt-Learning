+ export MASTER_PORT=12813
+ MASTER_PORT=12813
+ export NCCL_DEBUG=INFO
+ NCCL_DEBUG=INFO
+ export TORCH_DISTRIBUTED_DEBUG=DETAIL
+ TORCH_DISTRIBUTED_DEBUG=DETAIL
++ scontrol show hostnames devbox2
++ head -n 1
+ master_addr=devbox2
+ export MASTER_ADDR=devbox2
+ MASTER_ADDR=devbox2
+ export PYTHONPATH=:/nfs/home/tahmasebzadehg/prompt_learning/sbatch/src
+ PYTHONPATH=:/nfs/home/tahmasebzadehg/prompt_learning/sbatch/src
+ srun --cpu_bind=v --accel-bind=gn /nfs/home/tahmasebzadehg/miniconda3/envs/py310/bin/python -u /nfs/home/tahmasebzadehg/prompt_learning/src/training/main_coop.py --save-frequency 1 --zeroshot-frequency 1 --val-frequency 1 --batch-size=32 --workers=8 --precision amp --seed 0 --epochs 201 --data-dir /nfs/home/tahmasebzadehg/prompt_learning/data --pretrained openai --lr 2e-03 --wd 0.001 --dataset-name test --kg-init random --gt-label-name gt_label --CLASS-TOKEN-POSITION front --CSC True --model ViT-B-32 --N-CTX 16 --data_set_number 1 --pre-fname 2023_08_11_TEST
2023-08-11,10:49:45 | INFO | Running with a single process. Device cuda:0.
2023-08-11,10:49:45 | INFO | Loading pretrained ViT-B-32 from OpenAI.
Initializing class-specific contexts
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
2023-08-11,10:49:51 | INFO | Trainable Parameters backbone: 172032
2023-08-11,10:49:51 | INFO | Model:
2023-08-11,10:49:51 | INFO | COOPCLIP(
  (prompt_learner): PromptLearner()
  (image_encoder): VisualTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (text_encoder): TextEncoder(
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
      )
    )
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
2023-08-11,10:49:51 | INFO | Params:
2023-08-11,10:49:51 | INFO |   CLASS_TOKEN_POSITION: front
2023-08-11,10:49:51 | INFO |   CSC: True
2023-08-11,10:49:51 | INFO |   N_CTX: 16
2023-08-11,10:49:51 | INFO |   batch_size: 32
2023-08-11,10:49:51 | INFO |   beta1: 0.9
2023-08-11,10:49:51 | INFO |   beta2: 0.98
2023-08-11,10:49:51 | INFO |   checkpoint_path: ./logs/2023_08_11_TEST-init-random-model-ViT-B-32-ctx-16-front-shot-5-round-1/checkpoints
2023-08-11,10:49:51 | INFO |   class_names_path: /nfs/home/tahmasebzadehg/prompt_learning/data/test/class_names.csv
2023-08-11,10:49:51 | INFO |   copy_codebase: False
2023-08-11,10:49:51 | INFO |   csv_caption_key: title
2023-08-11,10:49:51 | INFO |   csv_img_key: filepath
2023-08-11,10:49:51 | INFO |   csv_separator: ,
2023-08-11,10:49:51 | INFO |   ctx_init: 
2023-08-11,10:49:51 | INFO |   data_dir: /nfs/home/tahmasebzadehg/prompt_learning/data
2023-08-11,10:49:51 | INFO |   data_set_number: 1
2023-08-11,10:49:51 | INFO |   dataset_name: test
2023-08-11,10:49:51 | INFO |   dataset_resampled: False
2023-08-11,10:49:51 | INFO |   dataset_type: auto
2023-08-11,10:49:51 | INFO |   ddp_static_graph: False
2023-08-11,10:49:51 | INFO |   debug: False
2023-08-11,10:49:51 | INFO |   device: cuda:0
2023-08-11,10:49:51 | INFO |   dist_backend: nccl
2023-08-11,10:49:51 | INFO |   dist_url: env://
2023-08-11,10:49:51 | INFO |   distribute_device: True
2023-08-11,10:49:51 | INFO |   distributed: False
2023-08-11,10:49:51 | INFO |   epochs: 201
2023-08-11,10:49:51 | INFO |   eps: 1e-06
2023-08-11,10:49:51 | INFO |   force_quick_gelu: False
2023-08-11,10:49:51 | INFO |   gather_with_grad: False
2023-08-11,10:49:51 | INFO |   grad_checkpointing: False
2023-08-11,10:49:51 | INFO |   gt_label_name: gt_label
2023-08-11,10:49:51 | INFO |   horovod: False
2023-08-11,10:49:51 | INFO |   image_mean: None
2023-08-11,10:49:51 | INFO |   image_std: None
2023-08-11,10:49:51 | INFO |   imagenet_v2: None
2023-08-11,10:49:51 | INFO |   imagenet_val: None
2023-08-11,10:49:51 | INFO |   kg_info_path: /nfs/home/tahmasebzadehg/prompt_learning/data/test/kg_info.json
2023-08-11,10:49:51 | INFO |   kg_init: random
2023-08-11,10:49:51 | INFO |   local_loss: False
2023-08-11,10:49:51 | INFO |   local_rank: 0
2023-08-11,10:49:51 | INFO |   lock_image: False
2023-08-11,10:49:51 | INFO |   lock_image_freeze_bn_stats: False
2023-08-11,10:49:51 | INFO |   lock_image_unlocked_groups: 0
2023-08-11,10:49:51 | INFO |   log_level: 20
2023-08-11,10:49:51 | INFO |   log_local: False
2023-08-11,10:49:51 | INFO |   log_path: ./logs/2023_08_11_TEST-init-random-model-ViT-B-32-ctx-16-front-shot-5-round-1/out.log
2023-08-11,10:49:51 | INFO |   logs: ./logs/
2023-08-11,10:49:51 | INFO |   lr: 0.002
2023-08-11,10:49:51 | INFO |   model: ViT-B-32
2023-08-11,10:49:51 | INFO |   n_round: 0
2023-08-11,10:49:51 | INFO |   name: 2023_08_11_TEST-init-random-model-ViT-B-32-ctx-16-front-shot-5-round-1
2023-08-11,10:49:51 | INFO |   no_set_device_rank: False
2023-08-11,10:49:51 | INFO |   no_train_backbone: True
2023-08-11,10:49:51 | INFO |   norm_gradient_clip: None
2023-08-11,10:49:51 | INFO |   only_last_layers: False
2023-08-11,10:49:51 | INFO |   pre_fname: 2023_08_11_TEST
2023-08-11,10:49:51 | INFO |   precision: amp
2023-08-11,10:49:51 | INFO |   pretrained: openai
2023-08-11,10:49:51 | INFO |   pretrained_image: False
2023-08-11,10:49:51 | INFO |   rank: 0
2023-08-11,10:49:51 | INFO |   report_to: 
2023-08-11,10:49:51 | INFO |   resume: None
2023-08-11,10:49:51 | INFO |   save_frequency: 1
2023-08-11,10:49:51 | INFO |   save_most_recent: False
2023-08-11,10:49:51 | INFO |   seed: 0
2023-08-11,10:49:51 | INFO |   shot: 5
2023-08-11,10:49:51 | INFO |   skip_scheduler: False
2023-08-11,10:49:51 | INFO |   tensorboard: False
2023-08-11,10:49:51 | INFO |   tensorboard_path: 
2023-08-11,10:49:51 | INFO |   torchscript: False
2023-08-11,10:49:51 | INFO |   trace: False
2023-08-11,10:49:51 | INFO |   train_data: /nfs/home/tahmasebzadehg/prompt_learning/data/test/train/round1/train_5_shot_round_1.csv
2023-08-11,10:49:51 | INFO |   train_gat: False
2023-08-11,10:49:51 | INFO |   train_num_samples: None
2023-08-11,10:49:51 | INFO |   train_prompts: True
2023-08-11,10:49:51 | INFO |   use_bn_sync: False
2023-08-11,10:49:51 | INFO |   val_data: /nfs/home/tahmasebzadehg/prompt_learning/data/test/val/round1/val_5_shot_round_1.csv
2023-08-11,10:49:51 | INFO |   val_frequency: 1
2023-08-11,10:49:51 | INFO |   val_num_samples: None
2023-08-11,10:49:51 | INFO |   wandb: False
2023-08-11,10:49:51 | INFO |   wandb_notes: 
2023-08-11,10:49:51 | INFO |   warmup: 10
2023-08-11,10:49:51 | INFO |   wd: 0.001
2023-08-11,10:49:51 | INFO |   workers: 8
2023-08-11,10:49:51 | INFO |   world_size: 1
2023-08-11,10:49:51 | INFO |   zeroshot_frequency: 1
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:49:51 | INFO | Start epoch 0
2023-08-11,10:49:55 | INFO | Train Epoch: 0 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 3.1545 (3.154) Time (t): 2.402 Batch (t): 3.917, 8.16883/s LR: 0.000667 Logit Scale: 100.000
2023-08-11,10:49:55 | INFO | Train Epoch: 0 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 2.9822 (3.068) Time (t): 0.085 Batch (t): 0.191, 174.442/s LR: 0.002000 Logit Scale: 100.000
2023-08-11,10:49:59 | INFO | Eval Epoch: 1 [32 / 105]	Loss: 2.489699	
2023-08-11,10:49:59 | INFO | Eval Epoch: 1 val_loss: 2.5288	@1: 39.2300	@5: 61.1100	@10: 78.8200	epoch: 1.0000	num_samples: 105.0000
2023-08-11,10:50:08 | INFO | Start epoch 1
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:50:09 | INFO | Train Epoch: 1 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 2.0424 (2.042) Time (t): 1.126 Batch (t): 1.234, 25.9280/s LR: 0.002000 Logit Scale: 100.000
2023-08-11,10:50:10 | INFO | Train Epoch: 1 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 1.7878 (1.915) Time (t): 0.357 Batch (t): 0.465, 44.5546/s LR: 0.002000 Logit Scale: 100.000
2023-08-11,10:50:12 | INFO | Eval Epoch: 2 [32 / 105]	Loss: 1.911555	
2023-08-11,10:50:12 | INFO | Eval Epoch: 2 val_loss: 1.9589	@1: 47.4800	@5: 78.8200	@10: 87.4100	epoch: 2.0000	num_samples: 105.0000
2023-08-11,10:50:21 | INFO | Start epoch 2
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:50:23 | INFO | Train Epoch: 2 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 1.0895 (1.089) Time (t): 1.992 Batch (t): 2.095, 15.2718/s LR: 0.002000 Logit Scale: 100.000
2023-08-11,10:50:23 | INFO | Train Epoch: 2 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 1.0268 (1.058) Time (t): 0.090 Batch (t): 0.196, 162.728/s LR: 0.002000 Logit Scale: 100.000
2023-08-11,10:50:25 | INFO | Eval Epoch: 3 [32 / 105]	Loss: 1.677382	
2023-08-11,10:50:26 | INFO | Eval Epoch: 3 val_loss: 1.7224	@1: 53.7300	@5: 81.1600	@10: 91.4000	epoch: 3.0000	num_samples: 105.0000
2023-08-11,10:50:34 | INFO | Start epoch 3
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:50:35 | INFO | Train Epoch: 3 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.65505 (0.6551) Time (t): 1.205 Batch (t): 1.310, 24.4258/s LR: 0.002000 Logit Scale: 100.000
2023-08-11,10:50:36 | INFO | Train Epoch: 3 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.55569 (0.6054) Time (t): 0.195 Batch (t): 0.300, 80.2821/s LR: 0.001999 Logit Scale: 100.000
2023-08-11,10:50:38 | INFO | Eval Epoch: 4 [32 / 105]	Loss: 1.657572	
2023-08-11,10:50:38 | INFO | Eval Epoch: 4 val_loss: 1.8175	@1: 47.7400	@5: 75.6900	@10: 87.8500	epoch: 4.0000	num_samples: 105.0000
2023-08-11,10:50:38 | INFO | Start epoch 4
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:50:40 | INFO | Train Epoch: 4 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.26273 (0.2627) Time (t): 2.017 Batch (t): 2.122, 15.0809/s LR: 0.001999 Logit Scale: 100.000
2023-08-11,10:50:41 | INFO | Train Epoch: 4 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.14198 (0.2024) Time (t): 0.097 Batch (t): 0.203, 158.181/s LR: 0.001998 Logit Scale: 100.000
2023-08-11,10:50:42 | INFO | Eval Epoch: 5 [32 / 105]	Loss: 1.844518	
2023-08-11,10:50:43 | INFO | Eval Epoch: 5 val_loss: 1.7414	@1: 54.8600	@5: 80.8100	@10: 87.8500	epoch: 5.0000	num_samples: 105.0000
2023-08-11,10:50:51 | INFO | Start epoch 5
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:50:53 | INFO | Train Epoch: 5 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.14776 (0.1478) Time (t): 1.666 Batch (t): 1.786, 17.9211/s LR: 0.001998 Logit Scale: 100.000
2023-08-11,10:50:54 | INFO | Train Epoch: 5 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.11603 (0.1319) Time (t): 0.095 Batch (t): 0.205, 162.763/s LR: 0.001997 Logit Scale: 100.000
2023-08-11,10:50:55 | INFO | Eval Epoch: 6 [32 / 105]	Loss: 1.960384	
2023-08-11,10:50:56 | INFO | Eval Epoch: 6 val_loss: 1.7209	@1: 61.2000	@5: 82.3800	@10: 88.6300	epoch: 6.0000	num_samples: 105.0000
slurmstepd: error: *** JOB 12061 ON devbox2 CANCELLED AT 2023-08-11T10:50:58 ***
slurmstepd: error: *** STEP 12061.0 ON devbox2 CANCELLED AT 2023-08-11T10:50:58 ***

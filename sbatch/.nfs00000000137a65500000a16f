+ export MASTER_PORT=12813
+ MASTER_PORT=12813
+ export NCCL_DEBUG=INFO
+ NCCL_DEBUG=INFO
+ export TORCH_DISTRIBUTED_DEBUG=DETAIL
+ TORCH_DISTRIBUTED_DEBUG=DETAIL
++ scontrol show hostnames devbox2
++ head -n 1
+ master_addr=devbox2
+ export MASTER_ADDR=devbox2
+ MASTER_ADDR=devbox2
+ export PYTHONPATH=:/nfs/home/tahmasebzadehg/prompt_learning/sbatch/src
+ PYTHONPATH=:/nfs/home/tahmasebzadehg/prompt_learning/sbatch/src
+ srun --cpu_bind=v --accel-bind=gn /nfs/home/tahmasebzadehg/miniconda3/envs/py310/bin/python -u /nfs/home/tahmasebzadehg/prompt_learning/src/training/main_coop.py --save-frequency 1 --zeroshot-frequency 1 --val-frequency 1 --batch-size=32 --workers=8 --precision amp --seed 0 --epochs 201 --data-dir /nfs/home/tahmasebzadehg/prompt_learning/data --pretrained openai --lr 2e-03 --wd 0.001 --dataset-name test --kg-init wikidata --gt-label-name gt_label --CLASS-TOKEN-POSITION front --CSC True --model ViT-B-32 --N-CTX 16 --data_set_number 1 --pre-fname 2023_08_11_TEST
2023-08-11,10:52:57 | INFO | Running with a single process. Device cuda:0.
2023-08-11,10:52:57 | INFO | Loading pretrained ViT-B-32 from OpenAI.
2023-08-11,10:53:04 | INFO | Trainable Parameters backbone: 172032
2023-08-11,10:53:04 | INFO | Model:
2023-08-11,10:53:04 | INFO | COOPCLIP(
  (prompt_learner): PromptLearner()
  (image_encoder): VisualTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (text_encoder): TextEncoder(
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ln_attn): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (ln): Identity()
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
        )
      )
    )
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
2023-08-11,10:53:04 | INFO | Params:
2023-08-11,10:53:04 | INFO |   CLASS_TOKEN_POSITION: front
2023-08-11,10:53:04 | INFO |   CSC: True
2023-08-11,10:53:04 | INFO |   N_CTX: 16
2023-08-11,10:53:04 | INFO |   batch_size: 32
2023-08-11,10:53:04 | INFO |   beta1: 0.9
2023-08-11,10:53:04 | INFO |   beta2: 0.98
2023-08-11,10:53:04 | INFO |   checkpoint_path: ./logs/2023_08_11_TEST-init-wikidata-model-ViT-B-32-ctx-16-front-shot-5-round-1/checkpoints
2023-08-11,10:53:04 | INFO |   class_names_path: /nfs/home/tahmasebzadehg/prompt_learning/data/test/class_names.csv
2023-08-11,10:53:04 | INFO |   copy_codebase: False
2023-08-11,10:53:04 | INFO |   csv_caption_key: title
2023-08-11,10:53:04 | INFO |   csv_img_key: filepath
2023-08-11,10:53:04 | INFO |   csv_separator: ,
2023-08-11,10:53:04 | INFO |   ctx_init: 
2023-08-11,10:53:04 | INFO |   data_dir: /nfs/home/tahmasebzadehg/prompt_learning/data
2023-08-11,10:53:04 | INFO |   data_set_number: 1
2023-08-11,10:53:04 | INFO |   dataset_name: test
2023-08-11,10:53:04 | INFO |   dataset_resampled: False
2023-08-11,10:53:04 | INFO |   dataset_type: auto
2023-08-11,10:53:04 | INFO |   ddp_static_graph: False
2023-08-11,10:53:04 | INFO |   debug: False
2023-08-11,10:53:04 | INFO |   device: cuda:0
2023-08-11,10:53:04 | INFO |   dist_backend: nccl
2023-08-11,10:53:04 | INFO |   dist_url: env://
2023-08-11,10:53:04 | INFO |   distribute_device: True
2023-08-11,10:53:04 | INFO |   distributed: False
2023-08-11,10:53:04 | INFO |   epochs: 201
2023-08-11,10:53:04 | INFO |   eps: 1e-06
2023-08-11,10:53:04 | INFO |   force_quick_gelu: False
2023-08-11,10:53:04 | INFO |   gather_with_grad: False
2023-08-11,10:53:04 | INFO |   grad_checkpointing: False
2023-08-11,10:53:04 | INFO |   gt_label_name: gt_label
2023-08-11,10:53:04 | INFO |   horovod: False
2023-08-11,10:53:04 | INFO |   image_mean: None
2023-08-11,10:53:04 | INFO |   image_std: None
2023-08-11,10:53:04 | INFO |   imagenet_v2: None
2023-08-11,10:53:04 | INFO |   imagenet_val: None
2023-08-11,10:53:04 | INFO |   kg_info_path: /nfs/home/tahmasebzadehg/prompt_learning/data/test/kg_info.json
2023-08-11,10:53:04 | INFO |   kg_init: wikidata
2023-08-11,10:53:04 | INFO |   local_loss: False
2023-08-11,10:53:04 | INFO |   local_rank: 0
2023-08-11,10:53:04 | INFO |   lock_image: False
2023-08-11,10:53:04 | INFO |   lock_image_freeze_bn_stats: False
2023-08-11,10:53:04 | INFO |   lock_image_unlocked_groups: 0
2023-08-11,10:53:04 | INFO |   log_level: 20
2023-08-11,10:53:04 | INFO |   log_local: False
2023-08-11,10:53:04 | INFO |   log_path: ./logs/2023_08_11_TEST-init-wikidata-model-ViT-B-32-ctx-16-front-shot-5-round-1/out.log
2023-08-11,10:53:04 | INFO |   logs: ./logs/
2023-08-11,10:53:04 | INFO |   lr: 0.002
2023-08-11,10:53:04 | INFO |   model: ViT-B-32
2023-08-11,10:53:04 | INFO |   n_round: 0
2023-08-11,10:53:04 | INFO |   name: 2023_08_11_TEST-init-wikidata-model-ViT-B-32-ctx-16-front-shot-5-round-1
2023-08-11,10:53:04 | INFO |   no_set_device_rank: False
2023-08-11,10:53:04 | INFO |   no_train_backbone: True
2023-08-11,10:53:04 | INFO |   norm_gradient_clip: None
2023-08-11,10:53:04 | INFO |   only_last_layers: False
2023-08-11,10:53:04 | INFO |   pre_fname: 2023_08_11_TEST
2023-08-11,10:53:04 | INFO |   precision: amp
2023-08-11,10:53:04 | INFO |   pretrained: openai
2023-08-11,10:53:04 | INFO |   pretrained_image: False
2023-08-11,10:53:04 | INFO |   rank: 0
2023-08-11,10:53:04 | INFO |   report_to: 
2023-08-11,10:53:04 | INFO |   resume: None
2023-08-11,10:53:04 | INFO |   save_frequency: 1
2023-08-11,10:53:04 | INFO |   save_most_recent: False
2023-08-11,10:53:04 | INFO |   seed: 0
2023-08-11,10:53:04 | INFO |   shot: 5
2023-08-11,10:53:04 | INFO |   skip_scheduler: False
2023-08-11,10:53:04 | INFO |   tensorboard: False
2023-08-11,10:53:04 | INFO |   tensorboard_path: 
2023-08-11,10:53:04 | INFO |   torchscript: False
2023-08-11,10:53:04 | INFO |   trace: False
2023-08-11,10:53:04 | INFO |   train_data: /nfs/home/tahmasebzadehg/prompt_learning/data/test/train/round1/train_5_shot_round_1.csv
2023-08-11,10:53:04 | INFO |   train_gat: False
2023-08-11,10:53:04 | INFO |   train_num_samples: None
2023-08-11,10:53:04 | INFO |   train_prompts: True
2023-08-11,10:53:04 | INFO |   use_bn_sync: False
2023-08-11,10:53:04 | INFO |   val_data: /nfs/home/tahmasebzadehg/prompt_learning/data/test/val/round1/val_5_shot_round_1.csv
2023-08-11,10:53:04 | INFO |   val_frequency: 1
2023-08-11,10:53:04 | INFO |   val_num_samples: None
2023-08-11,10:53:04 | INFO |   wandb: False
2023-08-11,10:53:04 | INFO |   wandb_notes: 
2023-08-11,10:53:04 | INFO |   warmup: 10
2023-08-11,10:53:04 | INFO |   wd: 0.001
2023-08-11,10:53:04 | INFO |   workers: 8
2023-08-11,10:53:04 | INFO |   world_size: 1
2023-08-11,10:53:04 | INFO |   zeroshot_frequency: 1
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:53:04 | INFO | Start epoch 0
2023-08-11,10:53:08 | INFO | Train Epoch: 0 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 3.0935 (3.093) Time (t): 1.802 Batch (t): 4.121, 7.76547/s LR: 0.000667 Logit Scale: 100.000
2023-08-11,10:53:08 | INFO | Train Epoch: 0 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 2.9773 (3.035) Time (t): 0.105 Batch (t): 0.211, 157.395/s LR: 0.002000 Logit Scale: 100.000
2023-08-11,10:53:11 | INFO | Eval Epoch: 1 [32 / 105]	Loss: 2.467594	
2023-08-11,10:53:11 | INFO | Eval Epoch: 1 val_loss: 2.4746	@1: 40.8000	@5: 67.0200	@10: 78.3900	epoch: 1.0000	num_samples: 105.0000
2023-08-11,10:53:19 | INFO | Start epoch 1
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:53:21 | INFO | Train Epoch: 1 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 1.9299 (1.930) Time (t): 1.943 Batch (t): 2.072, 15.4412/s LR: 0.002000 Logit Scale: 100.000
2023-08-11,10:53:22 | INFO | Train Epoch: 1 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 1.6617 (1.796) Time (t): 0.103 Batch (t): 0.211, 155.331/s LR: 0.002000 Logit Scale: 100.000
2023-08-11,10:53:23 | INFO | Eval Epoch: 2 [32 / 105]	Loss: 1.786931	
2023-08-11,10:53:24 | INFO | Eval Epoch: 2 val_loss: 1.8668	@1: 45.4800	@5: 74.0400	@10: 85.4200	epoch: 2.0000	num_samples: 105.0000
2023-08-11,10:53:32 | INFO | Start epoch 2
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:53:34 | INFO | Train Epoch: 2 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 1.0199 (1.020) Time (t): 1.737 Batch (t): 1.841, 17.3816/s LR: 0.002000 Logit Scale: 100.000
2023-08-11,10:53:35 | INFO | Train Epoch: 2 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.77591 (0.8979) Time (t): 0.096 Batch (t): 0.203, 155.799/s LR: 0.002000 Logit Scale: 100.000
2023-08-11,10:53:36 | INFO | Eval Epoch: 3 [32 / 105]	Loss: 1.385817	
2023-08-11,10:53:37 | INFO | Eval Epoch: 3 val_loss: 1.7157	@1: 51.3900	@5: 78.0400	@10: 90.1900	epoch: 3.0000	num_samples: 105.0000
2023-08-11,10:53:45 | INFO | Start epoch 3
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:53:47 | INFO | Train Epoch: 3 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.47040 (0.4704) Time (t): 1.453 Batch (t): 1.557, 20.5466/s LR: 0.002000 Logit Scale: 100.000
2023-08-11,10:53:47 | INFO | Train Epoch: 3 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.47624 (0.4733) Time (t): 0.117 Batch (t): 0.224, 142.444/s LR: 0.001999 Logit Scale: 100.000
2023-08-11,10:53:49 | INFO | Eval Epoch: 4 [32 / 105]	Loss: 1.381154	
2023-08-11,10:53:49 | INFO | Eval Epoch: 4 val_loss: 1.7771	@1: 47.4000	@5: 78.8200	@10: 89.4100	epoch: 4.0000	num_samples: 105.0000
2023-08-11,10:53:49 | INFO | Start epoch 4
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:53:51 | INFO | Train Epoch: 4 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.19781 (0.1978) Time (t): 1.692 Batch (t): 1.797, 17.8057/s LR: 0.001999 Logit Scale: 100.000
2023-08-11,10:53:52 | INFO | Train Epoch: 4 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.24325 (0.2205) Time (t): 0.099 Batch (t): 0.207, 159.932/s LR: 0.001998 Logit Scale: 100.000
2023-08-11,10:53:54 | INFO | Eval Epoch: 5 [32 / 105]	Loss: 1.546702	
2023-08-11,10:53:54 | INFO | Eval Epoch: 5 val_loss: 1.6697	@1: 54.8600	@5: 76.8200	@10: 91.7500	epoch: 5.0000	num_samples: 105.0000
2023-08-11,10:54:03 | INFO | Start epoch 5
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:54:05 | INFO | Train Epoch: 5 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.14128 (0.1413) Time (t): 2.083 Batch (t): 2.282, 14.0204/s LR: 0.001998 Logit Scale: 100.000
2023-08-11,10:54:05 | INFO | Train Epoch: 5 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.16195 (0.1516) Time (t): 0.102 Batch (t): 0.212, 149.206/s LR: 0.001997 Logit Scale: 100.000
2023-08-11,10:54:07 | INFO | Eval Epoch: 6 [32 / 105]	Loss: 1.766676	
2023-08-11,10:54:08 | INFO | Eval Epoch: 6 val_loss: 1.6608	@1: 59.9800	@5: 83.9400	@10: 93.7500	epoch: 6.0000	num_samples: 105.0000
2023-08-11,10:54:16 | INFO | Start epoch 6
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:54:18 | INFO | Train Epoch: 6 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.028016 (0.02802) Time (t): 1.376 Batch (t): 1.481, 21.6070/s LR: 0.001997 Logit Scale: 100.000
2023-08-11,10:54:18 | INFO | Train Epoch: 6 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.065838 (0.04693) Time (t): 0.100 Batch (t): 0.206, 155.549/s LR: 0.001996 Logit Scale: 100.000
2023-08-11,10:54:20 | INFO | Eval Epoch: 7 [32 / 105]	Loss: 1.694951	
2023-08-11,10:54:20 | INFO | Eval Epoch: 7 val_loss: 1.5325	@1: 64.3200	@5: 85.9400	@10: 94.5300	epoch: 7.0000	num_samples: 105.0000
2023-08-11,10:54:29 | INFO | Start epoch 7
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:54:31 | INFO | Train Epoch: 7 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.035046 (0.03505) Time (t): 1.254 Batch (t): 1.361, 23.5183/s LR: 0.001996 Logit Scale: 100.000
2023-08-11,10:54:31 | INFO | Train Epoch: 7 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.017745 (0.02640) Time (t): 0.104 Batch (t): 0.212, 156.720/s LR: 0.001995 Logit Scale: 100.000
2023-08-11,10:54:33 | INFO | Eval Epoch: 8 [32 / 105]	Loss: 1.601320	
2023-08-11,10:54:33 | INFO | Eval Epoch: 8 val_loss: 1.5272	@1: 64.6700	@5: 87.5000	@10: 96.1000	epoch: 8.0000	num_samples: 105.0000
2023-08-11,10:54:42 | INFO | Start epoch 8
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:54:45 | INFO | Train Epoch: 8 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.024230 (0.02423) Time (t): 2.477 Batch (t): 2.679, 11.9441/s LR: 0.001994 Logit Scale: 100.000
2023-08-11,10:54:45 | INFO | Train Epoch: 8 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.019762 (0.02200) Time (t): 0.102 Batch (t): 0.211, 154.385/s LR: 0.001993 Logit Scale: 100.000
2023-08-11,10:54:47 | INFO | Eval Epoch: 9 [32 / 105]	Loss: 1.572519	
2023-08-11,10:54:47 | INFO | Eval Epoch: 9 val_loss: 1.5504	@1: 63.8900	@5: 87.5000	@10: 93.7500	epoch: 9.0000	num_samples: 105.0000
2023-08-11,10:54:47 | INFO | Start epoch 9
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:54:49 | INFO | Train Epoch: 9 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.010402 (0.01040) Time (t): 1.201 Batch (t): 1.305, 24.5217/s LR: 0.001992 Logit Scale: 100.000
2023-08-11,10:54:50 | INFO | Train Epoch: 9 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.015256 (0.01283) Time (t): 0.281 Batch (t): 0.387, 145.715/s LR: 0.001991 Logit Scale: 100.000
2023-08-11,10:54:51 | INFO | Eval Epoch: 10 [32 / 105]	Loss: 1.569786	
2023-08-11,10:54:52 | INFO | Eval Epoch: 10 val_loss: 1.5500	@1: 66.6700	@5: 88.2800	@10: 92.9700	epoch: 10.0000	num_samples: 105.0000
2023-08-11,10:55:00 | INFO | Start epoch 10
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:55:02 | INFO | Train Epoch: 10 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.0082979 (0.008298) Time (t): 1.523 Batch (t): 1.628, 19.6542/s LR: 0.001990 Logit Scale: 100.000
2023-08-11,10:55:03 | INFO | Train Epoch: 10 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.0055838 (0.006941) Time (t): 0.415 Batch (t): 0.524, 144.933/s LR: 0.001988 Logit Scale: 100.000
2023-08-11,10:55:05 | INFO | Eval Epoch: 11 [32 / 105]	Loss: 1.586766	
2023-08-11,10:55:05 | INFO | Eval Epoch: 11 val_loss: 1.5775	@1: 61.5400	@5: 87.5000	@10: 94.5300	epoch: 11.0000	num_samples: 105.0000
2023-08-11,10:55:05 | INFO | Start epoch 11
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:55:07 | INFO | Train Epoch: 11 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.0077097 (0.007710) Time (t): 1.476 Batch (t): 1.583, 20.2101/s LR: 0.001988 Logit Scale: 100.000
2023-08-11,10:55:07 | INFO | Train Epoch: 11 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.0046094 (0.006160) Time (t): 0.094 Batch (t): 0.203, 159.127/s LR: 0.001986 Logit Scale: 100.000
2023-08-11,10:55:09 | INFO | Eval Epoch: 12 [32 / 105]	Loss: 1.620861	
2023-08-11,10:55:10 | INFO | Eval Epoch: 12 val_loss: 1.5987	@1: 57.9800	@5: 88.2800	@10: 94.5300	epoch: 12.0000	num_samples: 105.0000
2023-08-11,10:55:10 | INFO | Start epoch 12
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:55:11 | INFO | Train Epoch: 12 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.0046600 (0.004660) Time (t): 1.276 Batch (t): 1.383, 23.1384/s LR: 0.001985 Logit Scale: 100.000
2023-08-11,10:55:12 | INFO | Train Epoch: 12 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.0055592 (0.005110) Time (t): 0.175 Batch (t): 0.284, 151.718/s LR: 0.001983 Logit Scale: 100.000
2023-08-11,10:55:13 | INFO | Eval Epoch: 13 [32 / 105]	Loss: 1.622223	
2023-08-11,10:55:14 | INFO | Eval Epoch: 13 val_loss: 1.6071	@1: 57.9800	@5: 86.7200	@10: 94.5300	epoch: 13.0000	num_samples: 105.0000
2023-08-11,10:55:14 | INFO | Start epoch 13
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:55:15 | INFO | Train Epoch: 13 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.0034239 (0.003424) Time (t): 1.416 Batch (t): 1.521, 21.0327/s LR: 0.001982 Logit Scale: 100.000
2023-08-11,10:55:16 | INFO | Train Epoch: 13 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.0027102 (0.003067) Time (t): 0.124 Batch (t): 0.228, 160.598/s LR: 0.001980 Logit Scale: 100.000
2023-08-11,10:55:17 | INFO | Eval Epoch: 14 [32 / 105]	Loss: 1.601199	
2023-08-11,10:55:18 | INFO | Eval Epoch: 14 val_loss: 1.6018	@1: 57.9800	@5: 87.5000	@10: 94.5300	epoch: 14.0000	num_samples: 105.0000
2023-08-11,10:55:18 | INFO | Start epoch 14
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:55:19 | INFO | Train Epoch: 14 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.0022914 (0.002291) Time (t): 1.478 Batch (t): 1.583, 20.2138/s LR: 0.001979 Logit Scale: 100.000
2023-08-11,10:55:20 | INFO | Train Epoch: 14 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.0045924 (0.003442) Time (t): 0.093 Batch (t): 0.200, 156.321/s LR: 0.001977 Logit Scale: 100.000
2023-08-11,10:55:22 | INFO | Eval Epoch: 15 [32 / 105]	Loss: 1.597049	
2023-08-11,10:55:22 | INFO | Eval Epoch: 15 val_loss: 1.6012	@1: 59.5400	@5: 87.5000	@10: 95.3100	epoch: 15.0000	num_samples: 105.0000
2023-08-11,10:55:22 | INFO | Start epoch 15
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:55:23 | INFO | Train Epoch: 15 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.0021507 (0.002151) Time (t): 1.175 Batch (t): 1.280, 25.0013/s LR: 0.001976 Logit Scale: 100.000
2023-08-11,10:55:24 | INFO | Train Epoch: 15 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.0042522 (0.003201) Time (t): 0.378 Batch (t): 0.484, 146.493/s LR: 0.001974 Logit Scale: 100.000
2023-08-11,10:55:26 | INFO | Eval Epoch: 16 [32 / 105]	Loss: 1.586048	
2023-08-11,10:55:26 | INFO | Eval Epoch: 16 val_loss: 1.5874	@1: 59.5400	@5: 87.5000	@10: 95.3100	epoch: 16.0000	num_samples: 105.0000
2023-08-11,10:55:26 | INFO | Start epoch 16
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:55:28 | INFO | Train Epoch: 16 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.0029512 (0.002951) Time (t): 1.408 Batch (t): 1.522, 21.0288/s LR: 0.001972 Logit Scale: 100.000
2023-08-11,10:55:29 | INFO | Train Epoch: 16 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.0015767 (0.002264) Time (t): 0.444 Batch (t): 0.553, 157.699/s LR: 0.001970 Logit Scale: 100.000
2023-08-11,10:55:30 | INFO | Eval Epoch: 17 [32 / 105]	Loss: 1.571368	
2023-08-11,10:55:31 | INFO | Eval Epoch: 17 val_loss: 1.5661	@1: 62.3200	@5: 89.0600	@10: 96.8800	epoch: 17.0000	num_samples: 105.0000
2023-08-11,10:55:31 | INFO | Start epoch 17
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:55:32 | INFO | Train Epoch: 17 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.0022031 (0.002203) Time (t): 1.593 Batch (t): 1.701, 18.8130/s LR: 0.001969 Logit Scale: 100.000
2023-08-11,10:55:33 | INFO | Train Epoch: 17 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.0011238 (0.001663) Time (t): 0.089 Batch (t): 0.197, 162.356/s LR: 0.001966 Logit Scale: 100.000
2023-08-11,10:55:34 | INFO | Eval Epoch: 18 [32 / 105]	Loss: 1.558720	
2023-08-11,10:55:35 | INFO | Eval Epoch: 18 val_loss: 1.5511	@1: 65.1000	@5: 89.0600	@10: 96.8800	epoch: 18.0000	num_samples: 105.0000
2023-08-11,10:55:35 | INFO | Start epoch 18
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:55:37 | INFO | Train Epoch: 18 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.0014279 (0.001428) Time (t): 1.703 Batch (t): 1.808, 17.6944/s LR: 0.001965 Logit Scale: 100.000
2023-08-11,10:55:37 | INFO | Train Epoch: 18 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.0015658 (0.001497) Time (t): 0.090 Batch (t): 0.197, 160.650/s LR: 0.001962 Logit Scale: 100.000
2023-08-11,10:55:38 | INFO | Eval Epoch: 19 [32 / 105]	Loss: 1.544429	
2023-08-11,10:55:39 | INFO | Eval Epoch: 19 val_loss: 1.5422	@1: 65.8800	@5: 88.2800	@10: 96.8800	epoch: 19.0000	num_samples: 105.0000
2023-08-11,10:55:39 | INFO | Start epoch 19
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:55:40 | INFO | Train Epoch: 19 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.0017248 (0.001725) Time (t): 0.896 Batch (t): 1.001, 31.9716/s LR: 0.001960 Logit Scale: 100.000
2023-08-11,10:55:40 | INFO | Train Epoch: 19 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.0010522 (0.001388) Time (t): 0.240 Batch (t): 0.347, 162.564/s LR: 0.001957 Logit Scale: 100.000
2023-08-11,10:55:42 | INFO | Eval Epoch: 20 [32 / 105]	Loss: 1.530094	
2023-08-11,10:55:43 | INFO | Eval Epoch: 20 val_loss: 1.5400	@1: 65.8800	@5: 88.2800	@10: 96.8800	epoch: 20.0000	num_samples: 105.0000
2023-08-11,10:55:43 | INFO | Start epoch 20
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:55:45 | INFO | Train Epoch: 20 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.00099585 (0.0009958) Time (t): 1.660 Batch (t): 1.765, 18.1315/s LR: 0.001956 Logit Scale: 100.000
2023-08-11,10:55:45 | INFO | Train Epoch: 20 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.0012306 (0.001113) Time (t): 0.090 Batch (t): 0.198, 163.358/s LR: 0.001953 Logit Scale: 100.000
2023-08-11,10:55:47 | INFO | Eval Epoch: 21 [32 / 105]	Loss: 1.514882	
2023-08-11,10:55:47 | INFO | Eval Epoch: 21 val_loss: 1.5404	@1: 65.8900	@5: 88.2800	@10: 96.8800	epoch: 21.0000	num_samples: 105.0000
2023-08-11,10:55:47 | INFO | Start epoch 21
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:55:49 | INFO | Train Epoch: 21 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.0016145 (0.001614) Time (t): 1.776 Batch (t): 1.881, 17.0110/s LR: 0.001951 Logit Scale: 100.000
2023-08-11,10:55:50 | INFO | Train Epoch: 21 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.0010169 (0.001316) Time (t): 0.101 Batch (t): 0.491, 41.0601/s LR: 0.001948 Logit Scale: 100.000
2023-08-11,10:55:52 | INFO | Eval Epoch: 22 [32 / 105]	Loss: 1.509039	
2023-08-11,10:55:52 | INFO | Eval Epoch: 22 val_loss: 1.5473	@1: 65.8900	@5: 88.2800	@10: 96.8800	epoch: 22.0000	num_samples: 105.0000
2023-08-11,10:55:52 | INFO | Start epoch 22
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:55:54 | INFO | Train Epoch: 22 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.0013145 (0.001314) Time (t): 1.413 Batch (t): 1.522, 21.0293/s LR: 0.001946 Logit Scale: 100.000
2023-08-11,10:55:54 | INFO | Train Epoch: 22 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.00077236 (0.001043) Time (t): 0.093 Batch (t): 0.197, 162.282/s LR: 0.001943 Logit Scale: 100.000
2023-08-11,10:55:56 | INFO | Eval Epoch: 23 [32 / 105]	Loss: 1.509010	
2023-08-11,10:55:56 | INFO | Eval Epoch: 23 val_loss: 1.5534	@1: 65.8900	@5: 89.0600	@10: 96.8800	epoch: 23.0000	num_samples: 105.0000
2023-08-11,10:55:56 | INFO | Start epoch 23
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:55:58 | INFO | Train Epoch: 23 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.0011929 (0.001193) Time (t): 1.547 Batch (t): 1.653, 19.3643/s LR: 0.001941 Logit Scale: 100.000
2023-08-11,10:55:58 | INFO | Train Epoch: 23 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.00076500 (0.0009790) Time (t): 0.093 Batch (t): 0.198, 162.450/s LR: 0.001937 Logit Scale: 100.000
2023-08-11,10:56:00 | INFO | Eval Epoch: 24 [32 / 105]	Loss: 1.513350	
2023-08-11,10:56:00 | INFO | Eval Epoch: 24 val_loss: 1.5635	@1: 65.1100	@5: 89.8400	@10: 96.8800	epoch: 24.0000	num_samples: 105.0000
2023-08-11,10:56:00 | INFO | Start epoch 24
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:56:02 | INFO | Train Epoch: 24 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.00096686 (0.0009669) Time (t): 1.143 Batch (t): 1.247, 25.6676/s LR: 0.001935 Logit Scale: 100.000
2023-08-11,10:56:02 | INFO | Train Epoch: 24 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.00065991 (0.0008134) Time (t): 0.224 Batch (t): 0.330, 163.581/s LR: 0.001932 Logit Scale: 100.000
2023-08-11,10:56:04 | INFO | Eval Epoch: 25 [32 / 105]	Loss: 1.524461	
2023-08-11,10:56:04 | INFO | Eval Epoch: 25 val_loss: 1.5716	@1: 67.8800	@5: 88.2800	@10: 96.8800	epoch: 25.0000	num_samples: 105.0000
2023-08-11,10:56:13 | INFO | Start epoch 25
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:56:16 | INFO | Train Epoch: 25 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.00078219 (0.0007822) Time (t): 2.418 Batch (t): 2.617, 12.2276/s LR: 0.001930 Logit Scale: 100.000
2023-08-11,10:56:16 | INFO | Train Epoch: 25 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.00068852 (0.0007354) Time (t): 0.096 Batch (t): 0.207, 156.561/s LR: 0.001926 Logit Scale: 100.000
2023-08-11,10:56:18 | INFO | Eval Epoch: 26 [32 / 105]	Loss: 1.540063	
2023-08-11,10:56:18 | INFO | Eval Epoch: 26 val_loss: 1.5781	@1: 67.8800	@5: 88.2800	@10: 96.8800	epoch: 26.0000	num_samples: 105.0000
2023-08-11,10:56:27 | INFO | Start epoch 26
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:56:28 | INFO | Train Epoch: 26 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.00072786 (0.0007279) Time (t): 1.689 Batch (t): 1.801, 17.7705/s LR: 0.001924 Logit Scale: 100.000
2023-08-11,10:56:29 | INFO | Train Epoch: 26 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.00071674 (0.0007223) Time (t): 0.094 Batch (t): 0.203, 156.260/s LR: 0.001920 Logit Scale: 100.000
2023-08-11,10:56:31 | INFO | Eval Epoch: 27 [32 / 105]	Loss: 1.551450	
2023-08-11,10:56:31 | INFO | Eval Epoch: 27 val_loss: 1.5790	@1: 67.8800	@5: 88.2800	@10: 96.8800	epoch: 27.0000	num_samples: 105.0000
2023-08-11,10:56:40 | INFO | Start epoch 27
/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2023-08-11,10:56:42 | INFO | Train Epoch: 27 [ 32/105 (33%)] batch_count: 1 - batch_size 32 Loss: 0.00071435 (0.0007144) Time (t): 1.891 Batch (t): 2.000, 16.0025/s LR: 0.001918 Logit Scale: 100.000
2023-08-11,10:56:42 | INFO | Train Epoch: 27 [ 96/105 (100%)] batch_count: 3 - batch_size 32 Loss: 0.00083078 (0.0007726) Time (t): 0.090 Batch (t): 0.203, 153.468/s LR: 0.001914 Logit Scale: 100.000
2023-08-11,10:56:44 | INFO | Eval Epoch: 28 [32 / 105]	Loss: 1.561792	
2023-08-11,10:56:44 | INFO | Eval Epoch: 28 val_loss: 1.5795	@1: 67.8800	@5: 88.2800	@10: 96.8800	epoch: 28.0000	num_samples: 105.0000
Traceback (most recent call last):
  File "/nfs/home/tahmasebzadehg/prompt_learning/src/training/main_coop.py", line 369, in <module>
    device = main(args, _shot, args.data_set_number, device)
  File "/nfs/home/tahmasebzadehg/prompt_learning/src/training/main_coop.py", line 334, in main
    torch.save( checkpoint_dict,  os.path.join(args.checkpoint_path + "/chk.pt"),   )    
  File "/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/serialization.py", line 376, in save
    with _open_file_like(f, 'wb') as opened_file:
  File "/nfs/home/tahmasebzadehg/miniconda3/envs/py310/lib/python3.10/site-packages/torch/serialization.py", line 214, in __exit__
    self.file_like.close()
OSError: [Errno 116] Stale file handle
srun: error: devbox2: task 0: Exited with exit code 1
